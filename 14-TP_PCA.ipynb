{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP14 - Analisis de Componentes Principales (PCA)\n",
    "\n",
    "Un gran problema que surge cuando tratamos de realizar algún tipo de **aprendizaje automático** es la **maldición de la dimensionalidad**, la cual se refiere a los diversos fenómenos que surgen al analizar y organizar datos en espacios de múltiples dimensiones (cientos y miles de dimensiones) que no suceden en el espacio físico descrito generalmente con solo tres dimensiones.\n",
    "\n",
    "Sitio donde hay una buena explicación sobre el tema ([link](http://www.albertolumbreras.net/posts/maldicion-dimensionalidad.html)) \n",
    "\n",
    "El problema con las grandes dimensiones, es que cuanto mas dimensiones hay, todos los puntos quedan concentrados en un espacio muy pequeño. En otras palabras, todos los puntos estarán cerca. Esto se puede visualizar en la siguiente imagen:\n",
    "\n",
    "![betancourt_cubes](img/11-betancourt_cubes.png)\n",
    "\n",
    "Por esta razón, se han investigado técnicas, para poder proyectar datos en un espacio de dimensionalidad $D$ en otro espacio de dimensionalidad menor $M$ tal que $M\\leq D$. Una de las técnicas mas utilizadas realizar esto es el **Análisis de Componentes Principales (PCA)**.\n",
    "\n",
    "#### Principal Component Analysis (PCA)\n",
    "En PCA lo que se busca es encontrar una matriz de proyección $U$ tal que describir un set de datos en términos de nuevas variables, llamadas **componentes**, no correlacionadas. Estos componentes se ordenan por la cantidad de varianza que describen. Permitiendo seleccionar una cantidad $p$ de componentes principales para describir el dataset que explique el conjunto de datos.\n",
    "\n",
    "![pca](img/11-basic_pca.png)\n",
    "\n",
    "##### Algoritmo: \n",
    "\n",
    "Dato el conjunto de datos de entrenamiento $\\{x_1, \\dots, x_n\\}\\mid x_i \\in R^D$\n",
    "* Computar la media $\\bar{x}$ para cada dimensión y la matriz de covarianza $M$\n",
    "$$\\bar{x}=\\frac{1}{N}\\sum_{n=1}^N x_n\\qquad M=\\frac{1}{N-1}\\sum_{n=1}^N(x_n-\\bar{x})^T(x_n-\\bar{x})$$\n",
    "* Calcular los auto-valores $(\\lambda_i\\mid i\\in d)$ y auto-vectores $(u_i\\mid i\\in d)$ de la matriz de covarianza $M$\n",
    "* Ordenar los auto-vectores $u_i$ de manera decreciente con respecto a sus correspondientes auto-valores $\\lambda_i$.\n",
    "* Crear la matriz $U=(u_1,\\dots,u_d)$\n",
    "\n",
    "Para realizar la proyección al nuevo espacio ortogonal:\n",
    "$$x_n^d=(U^T(x_n-\\bar{x})^T)^T$$\n",
    "\n",
    "$x_n^d$ es la matriz de ejemplos transformados ortogonalmente. A partir de la misma, podemos descartar dimensiones para llevar nuestro dataset a un espacion de $p$ dimensiones. \n",
    "\n",
    "Para recontruir una aproximación de los datos originales\n",
    "$$\\tilde{x}_n\\approx x_n^pU^T+\\bar{x}$$\n",
    "\n",
    "El error cuadratico de reconstrucción es\n",
    "$$\\sum_{n=1}^N(x_n-\\tilde{x}_n)^2=(N-1)\\sum_{j=p+1}^d\\lambda_j$$\n",
    "\n",
    "donde los $\\lambda_j$ son los auto-valores descartados en la proyección.\n",
    "\n",
    "##### ¿Como elegir p?\n",
    "$$\\frac{\\sum_{k=1}^M\\lambda_k}{\\sum_{k=1}^D\\lambda_k}>thresh$$ \n",
    "tal que $(0\\leq thresh \\leq 1)$.\n",
    "\n",
    "El $min(M)$ que satisfaga la ecuación previa explicara el $thresh$ porciento de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios \n",
    "1) Implementar PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_train(X):       \n",
    "    # 1) Implementar PCA mediante descomposición de la matriz de \n",
    "    # covarianza muestral. Utilizar la función np.linalg.eigh\n",
    "    # para una solución numéricamente más estable. Los valores \n",
    "    # a retornar son una matriz con las componentes principales \n",
    "    # como columnas (U) y el vector medio (mean). \n",
    "    # NOTA: las columnas de U tienen que retornar en orden de \n",
    "    # \"importancia\" decreciente \n",
    "    #\n",
    "\n",
    "    \n",
    "    return U, mean\n",
    "\n",
    "def pca_project(X, U, mean, keep_dim=-1):\n",
    "    n_samples, n_dim = X.shape\n",
    "    assert n_dim == len(mean)\n",
    "    if keep_dim < 0:\n",
    "        keep_dim = n_dim\n",
    "\n",
    "    # 2) Implementar la proyección de puntos en X (filas)\n",
    "    # empleando los (U, mean) estimados a partir del conjunto \n",
    "    # de entrenamiento. El parámetro keep_dim es el número de\n",
    "    # componentes a considerar (proyección de n_dim a keep_dim \n",
    "    # dimensiones). Si este valor es menor a 0, la dimensionalidad \n",
    "    # del espacio de salida es la misma que el de entrada.\n",
    "    #X_proj = ...\n",
    "    \n",
    "    return X_proj\n",
    "\n",
    "def pca_restore(X_proj, U, mean):\n",
    "    n_samples, keep_dim = X_proj.shape\n",
    "    return X_proj.dot(U[:, :keep_dim].T) + mean.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Obtener el dataset asignado a cada alumno ([datasets](https://scikit-learn.org/stable/datasets/index.html))\n",
    "\n",
    "3) Aplicar PCA al dataset obtenido en el punto 2. La cantidad de dimensiones a conservar luego de la transformación ortogonal debe representar aproximadamente el 80% de la varianza de los datos.\n",
    "\n",
    "4) Implementar el predictor que corresponda (clasificación ó regresión) para generalizar sobre el dataset asignado. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
