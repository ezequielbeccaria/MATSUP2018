{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = 15, 8\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# visualización de las primeras n imágenes en formato mnist (imágenes vectorizadas de 28x28)\n",
    "def visu_mnist(X, n=10):    \n",
    "    visu = [X[i].reshape((28,28)) for i in range(n)]\n",
    "    visu = np.vstack([np.hstack(visu[:n//2]), np.hstack(visu[n//2:])])\n",
    "    visu = (visu - visu.min()) / (visu.max() - visu.min())\n",
    "    plt.imshow(visu, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "# Descargamos el dataset en caso de que no haya sido descargado previamente -> aprox. 56MB\n",
    "# Como alternativa el mismo puede ser descargado manualmente desde \n",
    "# https://drive.google.com/file/d/12E2XQSaVi-pQVCIxEPbgiZtTFo81Fo0K/view?usp=sharing\n",
    "mnist = fetch_mldata('MNIST original', data_home='/home/ezequiel/datasets/')\n",
    "\n",
    "# Dividimos el dataset en un conjunto de entrenamiento (80% de los datos) y test (20% restantes)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, train_size=0.8)\n",
    "print('{} train samples with {} dimensions'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('{} test samples with {} dimensions'.format(X_test.shape[0], X_test.shape[1]))\n",
    "    \n",
    "visu_mnist(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de un punto en el conjunto de entrenamiento\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_train[0])\n",
    "plt.imshow(X_train[0].reshape((28,28)), cmap='gray')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dimentional Reduction (Reducción de dimensionalidad)\n",
    "Un gran problema que surge cuando tratamos de realizar algún tipo de **aprendizaje automático** es la **maldición de la dimensionalidad**, la cual se refiere a los diversos fenómenos que surgen al analizar y organizar datos en espacios de múltiples dimensiones (cientos y miles de dimensiones) que no suceden en el espacio físico descrito generalmente con solo tres dimensiones.\n",
    "\n",
    "Sitio donde hay una buena explicación sobre el tema ([link](http://www.albertolumbreras.net/posts/maldicion-dimensionalidad.html)) \n",
    "\n",
    "El problema con las grandes dimensiones, es que cuanto mas dimensiones hay, todos los puntos quedan concentrados en un espacio muy pequeño. En otras palabras, todos los puntos estarán cerca. Esto se puede visualizar en la siguiente imagen:\n",
    "\n",
    "![betancourt_cubes](img/11-betancourt_cubes.png)\n",
    "\n",
    "Por esta razón, se han investigado técnicas, para poder proyectar datos en un espacio de dimensionalidad $D$ en otro espacio de dimensionalidad menor $M$ tal que $M\\leq D$. Una de las técnicas mas utilizadas realizar esto es el **Análisis de Componentes Principales (PCA)**.\n",
    "\n",
    "#### Principal Component Analysis (PCA)\n",
    "En PCA lo que se busca es encontrar una matriz de proyección $U$ tal que describir un set de datos en términos de nuevas variables, llamadas **componentes**, no correlacionadas. Estos componentes se ordenan por la cantidad de varianza que describen. Permitiendo seleccionar una cantidad $p$ de componentes principales para describir el dataset que explique el conjunto de datos.\n",
    "\n",
    "![pca](img/11-basic_pca.png)\n",
    "\n",
    "##### Algoritmo: \n",
    "\n",
    "Dato el conjunto de datos de entrenamiento $\\{x_1, \\dots, x_n\\}\\mid x_i \\in R^D$\n",
    "* Computar la media $\\bar{x}$ para cada dimensión y la matriz de covarianza $S$\n",
    "$$\\bar{x}=\\frac{1}{N}\\sum_{n=1}^N x_n\\qquad S=\\frac{1}{N-1}\\sum_{n=1}^N(x_n-\\bar{x})^T(x_n-\\bar{x})$$\n",
    "* Calcular los auto-valores $(\\lambda_i\\mid i\\in D)$ y auto-vectores $(u_i\\mid i\\in D)$ de la matriz de covarianza $S$\n",
    "* Ordenar los auto-vectores $u_i$ de manera decreciente con respecto a sus correspondientes auto-valores $\\lambda_i$.\n",
    "* Crear la matriz $U=(u_1,\\dots,u_D)$\n",
    "\n",
    "Para realizar la proyección al nuevo espacio ortogonal:\n",
    "$$x_n^D=(U^T(x_n-\\bar{x})^T)^T$$\n",
    "\n",
    "$x_n^D$ es la matriz de ejemplos transformados ortogonalmente. A partir de la misma, podemos descartar dimensiones para llevar nuestro dataset a un espacion de $M$ dimensiones. \n",
    "\n",
    "Para recontruir una aproximación de los datos originales\n",
    "$$\\tilde{x}_n\\approx x_n^MU^T+\\bar{x}$$\n",
    "\n",
    "El error cuadratico de reconstrucción es\n",
    "$$\\sum_{n=1}^N(x_n-\\tilde{x}_n)^2=(N-1)\\sum_{j=M+1}^D\\lambda_j$$\n",
    "\n",
    "donde los $\\lambda_j$ son los auto-valores descartados en la proyección.\n",
    "\n",
    "##### ¿Como elegir M?\n",
    "$$\\frac{\\sum_{k=1}^M\\lambda_k}{\\sum_{k=1}^D\\lambda_k}>thresh$$ \n",
    "tal que $(0\\leq thresh \\leq 1)$.\n",
    "\n",
    "El $min(M)$ que satisfaga la ecuación previa explicara el $thresh$ porciento de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proyecto los datos escalados con PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=128)\n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(pca.explained_variance_ratio_.shape[0]), pca.explained_variance_ratio_)\n",
    "plt.show()\n",
    "#print(pca.explained_variance_ratio_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizar las primeras componentes principales (columnas de U)\n",
    "visu_mnist(pca.components_, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, transformamos con PCA los datos de entrenamiento\n",
    "X_train_pca = pca.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios \n",
    "1) Implementar PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_train(X):       \n",
    "    # 1) Implementar PCA mediante descomposición de la matriz de \n",
    "    # covarianza muestral. Utilizar la función np.linalg.eigh\n",
    "    # para una solución numéricamente más estable. Los valores \n",
    "    # a retornar son una matriz con las componentes principales \n",
    "    # como columnas (U) y el vector medio (mean). \n",
    "    # NOTA: las columnas de U tienen que retornar en orden de \n",
    "    # \"importancia\" decreciente \n",
    "    #\n",
    "\n",
    "    \n",
    "    return U, mean\n",
    "\n",
    "def pca_project(X, U, mean, keep_dim=-1):\n",
    "    n_samples, n_dim = X.shape\n",
    "    assert n_dim == len(mean)\n",
    "    if keep_dim < 0:\n",
    "        keep_dim = n_dim\n",
    "\n",
    "    # 2) Implementar la proyección de puntos en X (filas)\n",
    "    # empleando los (U, mean) estimados a partir del conjunto \n",
    "    # de entrenamiento. El parámetro keep_dim es el número de\n",
    "    # componentes a considerar (proyección de n_dim a keep_dim \n",
    "    # dimensiones). Si este valor es menor a 0, la dimensionalidad \n",
    "    # del espacio de salida es la misma que el de entrada.\n",
    "    #X_proj = ...\n",
    "    \n",
    "    return X_proj\n",
    "\n",
    "def pca_restore(X_proj, U, mean):\n",
    "    n_samples, keep_dim = X_proj.shape\n",
    "    return X_proj.dot(U[:, :keep_dim].T) + mean.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Volver a entrenar un predictor para el dataset **MNIST original** utilizando la implementación propia de PCA y K-Means."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
